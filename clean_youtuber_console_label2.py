import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from transformers import ElectraTokenizer, ElectraForSequenceClassification
from transformers import get_linear_schedule_with_warmup, logging
import torch
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from torch.optim import AdamW
from tqdm import tqdm
import os
import sys

# 0. ë””ë°”ì´ìŠ¤ ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("ğŸš€ ì‚¬ìš©í•˜ëŠ” ì¥ì¹˜: ", device)

# 1. í•™ìŠµ ì‹œ ê²½ê³  ë©”ì„¸ì§€ ì œê±°
logging.set_verbosity_error()

# ==============================================================================
# 2. ë°ì´í„° ë¡œë“œ ë° ë¼ë²¨ ë³‘í•© (í•µì‹¬ ìˆ˜ì •)
# ==============================================================================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data")
TRAIN_FILE = os.path.join(DATA_DIR, "train_beep.tsv")

if not os.path.exists(TRAIN_FILE):
    print(f"âŒ ì˜¤ë¥˜: ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print("   'download_beep_data_check.py'ë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    sys.exit(1)

print(f"ğŸ“– ë°ì´í„° ì½ëŠ” ì¤‘... {TRAIN_FILE}")
dataset = pd.read_csv(TRAIN_FILE, sep='\t').dropna(axis=0)

# ------------------------------------------------------------------------------
# [ì´ì „ ì½”ë“œ ì£¼ì„ ì²˜ë¦¬] 3ê°€ì§€ ë¶„ë¥˜ (ì²­ì •/ëª¨ìš•/í˜ì˜¤)
# ------------------------------------------------------------------------------
# # ê¸°ì¡´ì—ëŠ” offensive(1)ì™€ hate(2)ë¥¼ êµ¬ë¶„í–ˆìŠµë‹ˆë‹¤.
# label_map = {'none': 0, 'offensive': 1, 'hate': 2}
# dataset['label_id'] = dataset['hate'].map(label_map)
# ------------------------------------------------------------------------------

# [ìˆ˜ì •ëœ ì½”ë“œ] 2ê°€ì§€ ë¶„ë¥˜ (ì²­ì •/ì•…ì„±)
# offensive(1)ì™€ hate(2)ë¥¼ ëª¨ë‘ 1(ì•…ì„±)ë¡œ í†µí•©í•˜ì—¬ ì •í™•ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.
label_map = {'none': 0, 'offensive': 1, 'hate': 1}
dataset['label_id'] = dataset['hate'].map(label_map)

text = list(dataset['comments'].values)
label = dataset['label_id'].values

print(f"\t * í•™ìŠµ ë°ì´í„° ìˆ˜: {len(text)}ê°œ")
# ë¼ë²¨ ë¶„í¬ í™•ì¸
print(f"\t * ë¼ë²¨ ë¶„í¬ (0:ì²­ì •, 1:ì•…ì„±): {dataset['label_id'].value_counts().to_dict()}")

# 3. í…ìŠ¤íŠ¸ í† í°í™”
model_name = 'monologg/koelectra-base-v3-discriminator'
print(f"\nâš™ï¸ í† í¬ë‚˜ì´ì € ë¡œë“œ ({model_name})...")
tokenizer = ElectraTokenizer.from_pretrained(model_name)

inputs = tokenizer(text, truncation=True, max_length=64, add_special_tokens=True,
                   padding="max_length")
input_ids = inputs['input_ids']
attention_mask = inputs['attention_mask']

# 4. ë°ì´í„° ë¶„ë¦¬
train_ids, val_ids, train_labels, val_labels = train_test_split(input_ids, label, test_size=0.2, random_state=2025)
train_masks, val_masks, _, _ = train_test_split(attention_mask, label, test_size=0.2, random_state=2025)

# 5. Dataloader
batch_size = 32
train_data = TensorDataset(torch.tensor(train_ids), torch.tensor(train_masks), torch.tensor(train_labels))
train_dataloader = DataLoader(train_data, sampler=RandomSampler(train_data), batch_size=batch_size)

val_data = TensorDataset(torch.tensor(val_ids), torch.tensor(val_masks), torch.tensor(val_labels))
val_dataloader = DataLoader(val_data, sampler=SequentialSampler(val_data), batch_size=batch_size)

# ==============================================================================
# 6. ëª¨ë¸ ì„¤ì • (num_labels ë³€ê²½)
# ==============================================================================
print(f"ğŸ¤– ëª¨ë¸ ë¡œë“œ ì¤‘...")

# ------------------------------------------------------------------------------
# [ì´ì „ ì½”ë“œ ì£¼ì„ ì²˜ë¦¬] 3ê°€ì§€ ë¶„ë¥˜ ëª¨ë¸
# ------------------------------------------------------------------------------
# model = ElectraForSequenceClassification.from_pretrained(model_name, num_labels=3)
# ------------------------------------------------------------------------------

# [ìˆ˜ì •ëœ ì½”ë“œ] 2ê°€ì§€ ë¶„ë¥˜ ëª¨ë¸ (Binary Classification)
model = ElectraForSequenceClassification.from_pretrained(model_name, num_labels=2)
model.to(device)

optimizer = AdamW(model.parameters(), lr=5e-5, eps=1e-08)
epochs = 3
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0,
                                            num_training_steps=len(train_dataloader) * epochs)

print("\nğŸ”¥ í•™ìŠµ ì‹œì‘! (ì²­ì • vs ì•…ì„± ì´ì§„ ë¶„ë¥˜)")
for e in range(epochs):
    # Training
    model.train()
    total_loss = 0
    progress_bar = tqdm(train_dataloader, desc=f"Epoch {e + 1}/{epochs} (Train)", leave=False)

    for batch in progress_bar:
        batch = tuple(t.to(device) for t in batch)
        model.zero_grad()
        outputs = model(input_ids=batch[0], attention_mask=batch[1], labels=batch[2])
        loss = outputs.loss
        total_loss += loss.item()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        scheduler.step()
        progress_bar.set_postfix({'loss': loss.item()})

    # Validation
    model.eval()
    val_preds, val_labels = [], []
    for batch in val_dataloader:
        batch = tuple(t.to(device) for t in batch)
        with torch.no_grad():
            outputs = model(input_ids=batch[0], attention_mask=batch[1])
        logits = outputs.logits
        preds = torch.argmax(logits, dim=1)
        val_preds.extend(preds.cpu().numpy())
        val_labels.extend(batch[2].cpu().numpy())

    acc = np.sum(np.array(val_preds) == np.array(val_labels)) / len(val_preds)
    print(f"   Epoch {e + 1}: Avg Loss {total_loss / len(train_dataloader):.4f} | Val Accuracy {acc:.4f}")

# ==============================================================================
# 7. ëª¨ë¸ ì €ì¥
# ==============================================================================
print("\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
save_path = os.path.join(BASE_DIR, "saved_model_hate")
if not os.path.exists(save_path):
    os.makedirs(save_path)

# [í•„ìˆ˜] í…ì„œ ì—°ì†ì„± ë³´ì¥
for param in model.parameters():
    if not param.is_contiguous():
        param.data = param.data.contiguous()

model.save_pretrained(save_path)
tokenizer.save_pretrained(save_path)
print(f"âœ… ì €ì¥ ì™„ë£Œ! ê²½ë¡œ: {save_path}")
