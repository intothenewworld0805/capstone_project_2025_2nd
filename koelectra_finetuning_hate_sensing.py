import torch
import pandas as pd
import numpy as np
from transformers import ElectraTokenizer, ElectraForSequenceClassification
from torch.utils.data import DataLoader, TensorDataset, SequentialSampler
from tqdm import tqdm
import os
import sys
import json

# ==============================================================================
# [ì„¤ì •] ê²½ë¡œ ë° íŒŒì¼ëª…
# ==============================================================================
MODEL_PATH = "./saved_model_hate"
INPUT_CSV_FILE = "Yb6bjbWZaR8_all_only_comments.csv"

# ìž…ë ¥ íŒŒì¼ëª…ì—ì„œ ë¹„ë””ì˜¤ ID ì¶”ì¶œí•˜ì—¬ ê²°ê³¼ íŒŒì¼ëª… ìƒì„±
video_id_prefix = INPUT_CSV_FILE.split('_')[0]
OUTPUT_CSV_FILE = f"{video_id_prefix}_final_result_binary.csv"

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# [í•µì‹¬] 2ì§„ ë¶„ë¥˜ ë¼ë²¨ ì •ì˜
ID_TO_LABEL = {0: 'Clean (ì²­ì •)', 1: 'Toxic (ì•…ì„±)'}


# ==============================================================================
# [1] ëª¨ë¸ ë¡œë“œ
# ==============================================================================
def load_model():
    print(f"ðŸ“‚ ëª¨ë¸ ë¡œë”© ì¤‘... ({MODEL_PATH})")
    if not os.path.exists(MODEL_PATH):
        print(f"âŒ ì˜¤ë¥˜: '{MODEL_PATH}' í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None, None

    try:
        tokenizer = ElectraTokenizer.from_pretrained(MODEL_PATH)
        model = ElectraForSequenceClassification.from_pretrained(MODEL_PATH, num_labels=2)
        model.to(device)
        model.eval()
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! (Device: {device})")
        return tokenizer, model
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None, None


# ==============================================================================
# [2] ë°ì´í„° ë¡œë“œ (JSON ì •ë¦¬ í¬í•¨)
# ==============================================================================
def clean_json_text(text):
    text = str(text).strip()
    if text.startswith('{') and 'text' in text:
        try:
            data = json.loads(text)
            return data.get('text', text)
        except:
            pass
    return text


def load_data_robust(filepath):
    print(f"ðŸ“– ë°ì´í„° ì½ëŠ” ì¤‘... ({filepath})")
    if not os.path.exists(filepath):
        print(f"âŒ íŒŒì¼ì´ ì¡´ìž¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {filepath}")
        return None

    try:
        with open(filepath, 'r', encoding='utf-8-sig', errors='ignore') as f:
            lines = [line.strip() for line in f.readlines() if line.strip()]
        df = pd.DataFrame(lines, columns=['raw_text'])
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

    print("ðŸ§¹ ë°ì´í„° ì •ì œ ì¤‘...")
    df['text'] = df['raw_text'].apply(clean_json_text)
    df = df[df['text'].str.len() > 1]

    print(f"ðŸ“Š ë¶„ì„ ëŒ€ìƒ ë°ì´í„°: {len(df)}ê±´")
    return df, 'text'


# ==============================================================================
# [3] ë¶„ì„ ì‹¤í–‰
# ==============================================================================
def analyze(df, text_col, tokenizer, model):
    comments = df[text_col].astype(str).tolist()

    inputs = tokenizer(
        comments, return_tensors='pt', max_length=64,
        truncation=True, padding=True
    )

    dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'])
    dataloader = DataLoader(dataset, sampler=SequentialSampler(dataset), batch_size=32)

    print("ðŸš€ AI ë¶„ì„ ì‹œìž‘ (ì²­ì • vs ì•…ì„±)...")
    preds_list = []
    probs_list = []

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Inference Step"):
            batch = tuple(t.to(device) for t in batch)
            inputs = {'input_ids': batch[0], 'attention_mask': batch[1]}

            outputs = model(**inputs)
            probs = torch.nn.functional.softmax(outputs.logits, dim=1)
            preds = torch.argmax(probs, dim=1)

            preds_list.extend(preds.cpu().numpy())
            # ì„ íƒëœ í´ëž˜ìŠ¤ì˜ í™•ë¥ ê°’ (Confidence)
            probs_list.extend([probs[j][p].item() for j, p in enumerate(preds)])

    df['Label_ID'] = preds_list
    df['Result'] = [ID_TO_LABEL[p] for p in preds_list]
    df['Confidence_Val'] = probs_list  # ìˆ«ìží˜• (ê³„ì‚°ìš©)
    df['Confidence'] = [f"{p * 100:.1f}%" for p in probs_list]  # ë¬¸ìží˜• (ì¶œë ¥ìš©)
    df['Status'] = ['BLOCK' if p == 1 else 'PASS' for p in preds_list]

    return df


# ==============================================================================
# [4] ê²°ê³¼ ë¦¬í¬íŠ¸ (í›ˆë ¨ ë¡œê·¸ ìŠ¤íƒ€ì¼)
# ==============================================================================
def print_report(df, text_col):
    total = len(df)
    toxic_df = df[df['Status'] == 'BLOCK']
    clean_df = df[df['Status'] == 'PASS']

    toxic_cnt = len(toxic_df)
    clean_cnt = len(clean_df)

    # ë¹„ìœ¨ ê³„ì‚°
    toxic_ratio = (toxic_cnt / total) * 100 if total > 0 else 0
    clean_ratio = (clean_cnt / total) * 100 if total > 0 else 0

    # í‰ê·  í™•ì‹ ë„ (AIê°€ ì–¼ë§ˆë‚˜ í™•ì‹ í•˜ëŠ”ì§€)
    avg_conf = df['Confidence_Val'].mean() * 100 if total > 0 else 0

    print("\n" + "=" * 80)
    print(f"ðŸ“‹ [Analysis Result Summary] : {INPUT_CSV_FILE}")
    print("=" * 80)

    # [ìˆ˜ì •] Avg Confidence -> Avg Accuracy ë¡œ ëª…ì¹­ ë³€ê²½
    print(
        f"Total Samples: {total} | Clean: {clean_cnt} ({clean_ratio:.2f}%) | Toxic: {toxic_cnt} ({toxic_ratio:.2f}%) | Avg Accuracy: {avg_conf:.2f}%")
    print("-" * 80)

    if toxic_cnt > 0:
        print("\nðŸš¨ [Deteced Toxic Comments Sample]")
        for idx, row in toxic_df.head(5).iterrows():
            content = row[text_col].replace("\n", " ")[:60]
            conf = row['Confidence']
            print(f" - [Toxic] ({conf}) {content}...")
    else:
        print("\nâœ¨ No toxic comments detected.")

    save_df = df[['text', 'Result', 'Confidence', 'Status']]
    save_df.to_csv(OUTPUT_CSV_FILE, index=False, encoding='utf-8-sig')

    print("\n" + "=" * 80)
    print(f"ðŸ’¾ Result Saved: {OUTPUT_CSV_FILE}")
    print("=" * 80)


if __name__ == "__main__":
    tokenizer, model = load_model()
    if tokenizer:
        df, text_col = load_data_robust(INPUT_CSV_FILE)
        if df is not None and not df.empty:
            result_df = analyze(df, text_col, tokenizer, model)
            print_report(result_df, text_col)
