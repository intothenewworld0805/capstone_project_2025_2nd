import torch
import pandas as pd
import numpy as np
from transformers import ElectraTokenizer, ElectraForSequenceClassification
from torch.utils.data import DataLoader, TensorDataset, SequentialSampler
from tqdm import tqdm
import os
import sys
import json

# ==============================================================================
# [ì„¤ì •] ê²½ë¡œ ë° íŒŒì¼ëª…
# ==============================================================================
MODEL_PATH = "./saved_model_hate"
INPUT_CSV_FILE = "Yb6bjbWZaR8_all_only_comments.csv"
OUTPUT_CSV_FILE = "Yb6bjbWZaR8_final_console_result.csv"

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
ID_TO_LABEL = {0: 'Clean (ì²­ì •)', 1: 'Offensive (ëª¨ìš•)', 2: 'Hate (í˜ì˜¤)'}


# ==============================================================================
# [1] ëª¨ë¸ ë¡œë“œ
# ==============================================================================
def load_model():
    print(f"ğŸ“‚ ëª¨ë¸ ë¡œë”© ì¤‘... ({MODEL_PATH})")
    if not os.path.exists(MODEL_PATH):
        print(f"âŒ ì˜¤ë¥˜: '{MODEL_PATH}' í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None, None

    try:
        tokenizer = ElectraTokenizer.from_pretrained(MODEL_PATH)
        model = ElectraForSequenceClassification.from_pretrained(MODEL_PATH)
        model.to(device)
        model.eval()
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! (Device: {device})")
        return tokenizer, model
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None, None


# ==============================================================================
# [2] ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ (JSON ë¬¸ìì—´ ì •ë¦¬ í¬í•¨)
# ==============================================================================
def clean_json_text(text):
    """
    '{"text": "ì‹¤ì œ ë‚´ìš©"}' í˜•íƒœì˜ ë¬¸ìì—´ì—ì„œ ì‹¤ì œ ë‚´ìš©ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    text = str(text).strip()
    # JSON í˜•ì‹ì¸ ê²½ìš° íŒŒì‹± ì‹œë„
    if text.startswith('{') and 'text' in text:
        try:
            data = json.loads(text)
            return data.get('text', text)
        except:
            pass
    return text


def load_data_robust(filepath):
    print(f"ğŸ“– ë°ì´í„° ì½ëŠ” ì¤‘... ({filepath})")
    if not os.path.exists(filepath):
        print(f"âŒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {filepath}")
        return None

    # 1. í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì½ì–´ì„œ ê°•ì œë¡œ DataFrame ìƒì„± (ê°€ì¥ ì•ˆì „)
    try:
        with open(filepath, 'r', encoding='utf-8-sig', errors='ignore') as f:
            lines = [line.strip() for line in f.readlines() if line.strip()]
        df = pd.DataFrame(lines, columns=['raw_text'])
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

    # 2. JSON ë¬¸ìì—´ ì •ë¦¬ (í•µì‹¬ ê¸°ëŠ¥ ì¶”ê°€)
    print("ğŸ§¹ ë°ì´í„° ì •ì œ ì¤‘ (JSON íƒœê·¸ ì œê±°)...")
    df['text'] = df['raw_text'].apply(clean_json_text)

    # ë„ˆë¬´ ì§§ì€ ê¸€ ì œê±°
    df = df[df['text'].str.len() > 1]

    print(f"ğŸ“Š ë¶„ì„ ëŒ€ìƒ ë°ì´í„°: {len(df)}ê±´")
    return df, 'text'


# ==============================================================================
# [3] ë¶„ì„ ì‹¤í–‰
# ==============================================================================
def analyze(df, text_col, tokenizer, model):
    comments = df[text_col].astype(str).tolist()

    inputs = tokenizer(
        comments, return_tensors='pt', max_length=64,
        truncation=True, padding=True
    )

    dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'])
    dataloader = DataLoader(dataset, sampler=SequentialSampler(dataset), batch_size=32)

    print("ğŸš€ AI ë¶„ì„ ì‹œì‘...")
    preds_list = []
    probs_list = []

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Processing"):
            batch = tuple(t.to(device) for t in batch)
            inputs = {'input_ids': batch[0], 'attention_mask': batch[1]}

            outputs = model(**inputs)
            probs = torch.nn.functional.softmax(outputs.logits, dim=1)
            preds = torch.argmax(probs, dim=1)

            preds_list.extend(preds.cpu().numpy())
            probs_list.extend([probs[j][p].item() for j, p in enumerate(preds)])

    df['Label_ID'] = preds_list
    df['Result'] = [ID_TO_LABEL[p] for p in preds_list]
    df['Confidence'] = [f"{p * 100:.1f}%" for p in probs_list]
    df['Status'] = ['BLOCK' if p != 0 else 'PASS' for p in preds_list]

    return df


# ==============================================================================
# [4] ê²°ê³¼ ë¦¬í¬íŠ¸ ì¶œë ¥
# ==============================================================================
def print_report(df, text_col):
    total = len(df)
    toxic_df = df[df['Status'] == 'BLOCK']
    toxic_cnt = len(toxic_df)
    clean_cnt = total - toxic_cnt
    clean_score = (clean_cnt / total) * 100 if total > 0 else 0

    print("\n" + "=" * 60)
    print(f"ğŸ“‹ [ë¶„ì„ ê²°ê³¼ ë¦¬í¬íŠ¸] : {INPUT_CSV_FILE}")
    print("=" * 60)
    print(f"ğŸ”¹ ì´ ëŒ“ê¸€ ìˆ˜    : {total}ê°œ")
    print(f"ğŸŸ¢ ì²­ì • ëŒ“ê¸€     : {clean_cnt}ê°œ")
    print(f"ğŸ”´ ì•…ì„± ëŒ“ê¸€     : {toxic_cnt}ê°œ")
    print(f"ğŸ›¡ï¸ ì±„ë„ ì²­ì • ì§€ìˆ˜: {clean_score:.1f}ì ")
    print("-" * 60)

    print("\nğŸ”¢ [ìœ í˜•ë³„ ë¶„í¬]")
    print(df['Result'].value_counts().to_string())

    if toxic_cnt > 0:
        print("\nğŸš¨ [ê²€ì¶œëœ ì•…ì„± ëŒ“ê¸€ ìƒ˜í”Œ (ìµœëŒ€ 5ê°œ)]")
        print("-" * 60)
        # í…ìŠ¤íŠ¸ ê¸¸ì´ì œí•œì„ ë‘ê³  ê¹”ë”í•˜ê²Œ ì¶œë ¥
        for idx, row in toxic_df.head(5).iterrows():
            clean_content = row[text_col].replace("\n", " ")[:60]
            print(f"[{row['Result']}] {clean_content}...")
    else:
        print("\nâœ¨ ì•…ì„± ëŒ“ê¸€ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    # ì €ì¥ ì‹œì—ëŠ” ë³´ê¸° ì¢‹ê²Œ í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì €ì¥
    save_df = df[['text', 'Result', 'Confidence', 'Status']]
    save_df.to_csv(OUTPUT_CSV_FILE, index=False, encoding='utf-8-sig')

    print("\n" + "=" * 60)
    print(f"ğŸ’¾ ê¹”ë”í•˜ê²Œ ì •ë¦¬ëœ ê²°ê³¼ê°€ '{OUTPUT_CSV_FILE}' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("=" * 60)


if __name__ == "__main__":
    tokenizer, model = load_model()
    if tokenizer:
        df, text_col = load_data_robust(INPUT_CSV_FILE)
        if df is not None and not df.empty:
            result_df = analyze(df, text_col, tokenizer, model)
            print_report(result_df, text_col)
